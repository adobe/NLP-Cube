{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced usage - Model export and import\n",
    "\n",
    "Following the previous [model training tutorial] () we now have a working model located in ``/work/my_model-1.0``. We'll learn how to export a model as a zip file and then import it locally.\n",
    "\n",
    "\n",
    "## A look at NLP-Cube's model storage format\n",
    "\n",
    "NLP-Cube's model library is located in the hidden ``.nlpcube`` folder located in the user's home directory. This \"root\" folder contains the ``embeddings`` folder where all embeddings are stored, as well as the ``models`` folder in which any number of other sub-folders, each in the format ``model_name-model_version``, where the actual models are stored. This is the reason why we created a **``my_model-1.0``** folder: the part before the hyphen is the **model's name** we'll call during runtime, and **1.0** is the version of this model. This way we can have several versions of models for a language, and using NLP-Cube's .load() API call with the default version parameter will ensure we'll always load the latest version for any given model.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "~/.nlpcube/\n",
    "    /embeddings\n",
    "        wiki.en.vec\n",
    "        wiki.hr.vec\n",
    "        wiki.dummy.vec\n",
    "    /models\n",
    "        /en-1.0\n",
    "            model files here\n",
    "        /en-1.1\n",
    "            model files here\n",
    "        /hr-1.0\n",
    "            model files here\n",
    "        /my_model-1.0\n",
    "            model files here\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "There are only two rules for model naming: \n",
    "1. The name can only contain a single hyphen that divides the name from the version.\n",
    "2. The version must be a number (with or without a comma)\n",
    "\n",
    "We first look at how to export a locally-trained model, then how to import it, and finally we look in how we can manage our model library programatically.\n",
    "\n",
    "## 1. Export a locally-trained model \n",
    "\n",
    "There are two steps to package a model: first we need to create a metadata file which will let NLP-Cube instances know about the model that will be loaded (like path to embeddings, etc.) and secondly zip everything in a single file. \n",
    "\n",
    "### 1.1. Create metadata for the model\n",
    "\n",
    "In ``NLP-Cube/examples`` we have a ``metadata.json`` template file. This file is used by NLP-Cube to store data about a packaged model that we use and can further redistribute. Copy this file to ``my_model-1.0`` and edit it. Let's say we edited the file as:\n",
    "```\n",
    "{\n",
    "    \"embeddings_file_name\": \"wiki.en.vec\",\n",
    "    \"embeddings_remote_link\": \"https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec\",\n",
    "    \"language\": \"UD_English\",\n",
    "    \"language_code\": \"my_model\",\n",
    "    \"model_build_date\": \"2018-10-19\",\n",
    "    \"model_build_source\": \"UD_English-ParTuT\",\n",
    "    \"model_version\": 1.0,\n",
    "    \"notes\": \"Source: ud-treebanks-v2.2\",\n",
    "    \"token_delimiter\": \" \"\n",
    "}\n",
    "```\n",
    "Let's look at each line in detail (in bold where non-optional parameters are required):\n",
    "1. **``embeddings_file_name``** will be the local file name of the embeddings. This can be shared among versions and models so as not to keep multiple copies of the same embedding. If in doubt, just keep the original file name.\n",
    "2. **``embeddings_remote_link``** is the web link for NLP-Cube to automatically download an embedding file. As we only want to distribute the model and not the (very large!) embedding file, use this to point to an online file - in this case we're using the FastText wiki embeddings for English; otherwise leave empty, but be sure to manually copy the embedding file to ``/home/_your-username_/.nlpcube/embeddings/`` and give it the same name as ``embeddings_file_name``.\n",
    "4. **``language_code``** is the short code for the model.\n",
    "5. **``model_version``** is the version (read as a float number) of the model. Note that the model will be named **``language_code-model_version.zip``**, and will be loaded as ``.load(\"language_code\")``.\n",
    "6. **``token_delimiter``** is used to differentiate between languages that use space as a delimiter of words and those which don't (e.g. Japanese, Chinese). Set this to \"\" (empty string) for these particular languages, otherwise keep a singe space as in the default example. \n",
    "7. ``language``, ``model_build_date``, ``model_build_source``,  and ``notes`` are for reference purposes. Set them to whatever you like, and access them in python with the metadata.info() function.\n",
    "\n",
    "\n",
    "### 1.2. Export the model as a zip file\n",
    "\n",
    "Run the ``export_model.py`` script located in the ``NLP-Cube/scripts`` folder.\n",
    "As in the previous tutorial we trained only a tokenizer and a tagger, we'll only pass the --tokenizer and --tagger flags (so the packager won't look for other components), along with the path to our model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dynet] random seed: 2748650582\n",
      "[dynet] allocating memory: 512MB\n",
      "[dynet] memory allocation done.\n",
      "Usage: python3 export_model.py path-to-my-model --tokenizer(optional) --compound-word-expander(optional) --lemmatizer(optional) --tagger(optional) --parser(optional)\n",
      "Example: 'python3 export_model.py path-to-my-model --tokenizer --tagger' will create a zip file named 'language_code-model_version.zip' (taken from the metadata.json) containing a tokenizer and a tagger.\n",
      "\n",
      "\tModel folder: /work/my_model-1.0\n",
      "\tUse tokenizer: True\n",
      "\tUse compound word expander: False\n",
      "\tUse lemmatizer: False\n",
      "\tUse tagger: True\n",
      "\tUse parser: False\n",
      "\n",
      "\tWriting model to temp folder: /tmp/tmpgr_njhs5/my_model-1.0\n",
      "\tTokenizer model found.\n",
      "\tTagger model found.\n",
      "\tCompressing model ...\n",
      "\tCleaning up ...\n",
      "Model packaged successfully as: /work/my_model-1.0.zip\n"
     ]
    }
   ],
   "source": [
    "! python3 /work/NLP-Cube/scripts/export_model.py /work/my_model-1.0 --tokenizer --tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a ``my_model-1.0.zip`` file located in ``/work``. This file can now be redistributed to anybody to import and use. _Note the embeddings that should be either online or manually copied locally (details above)._\n",
    "\n",
    "\n",
    "## 2. Import a .zip model\n",
    "\n",
    "We just received a .zip model and we want to import it in NLP-Cube's model library. In ``NLP-Cube/scripts`` run the ``import_model.py`` file with the path to the .zip file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing model /work/my_model-1.0.zip\n",
      "\n",
      "Model extracted successfully.\n",
      "Checking for associated vector embeddings file [wiki.en.vec] ...\n",
      "\n",
      "Model /work/my_model-1.0.zip was successfully imported.\n"
     ]
    }
   ],
   "source": [
    "! cd /work/NLP-Cube/scripts && python3 import_model.py /work/my_model-1.0.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model is imported and from now on we can load it by simply calling ``cube.load(\"my_model\")``\n",
    "\n",
    "## 3. Manage NLP-Cube's model library programatically\n",
    "\n",
    "Either modify the contents of ``~/.nlpcube`` manually, or import a ModelStore object and instantiate it:\n",
    "\n",
    "```\n",
    "from cube.io_utils.model_store import ModelStore\n",
    "model_store_object = ModelStore()\n",
    "```\n",
    "\n",
    "To **view local models**, run:\n",
    "```\n",
    "local_models = model_store_object.list_local_models()\n",
    "print(local_models)\n",
    "```\n",
    "will yield a list of tuples like (lang_code, version):\n",
    "```\n",
    "[('en', 1.1), ('my_model',1.0)]\n",
    "```\n",
    "\n",
    "To **view available online models**, run:\n",
    "```\n",
    "online_models = model_store_object.list_online_models()\n",
    "print(online_models)\n",
    "```\n",
    "to see the current languages available in the cloud:\n",
    "```\n",
    "[('af', 1.0), ('ar', 1.0), ('bg', 1.0), ('bxr', 1.0), ('ca', 1.0), ('cs', 1.0), ('cu', 1.0), ('da', 1.0), ('de', 1.0), ('el', 1.0), ('en', 1.0), ('en', 1.1), ('en_small', 1.0), ('es', 1.0), ('et', 1.0), ('eu', 1.0), ('fa', 1.0), ('fi', 1.0), ('fr', 1.0), ('ga', 1.0), ('gl', 1.0), ('got', 1.0), ('grc', 1.0), ('he', 1.0), ('hi', 1.0), ('hr', 1.0), ('hsb', 1.0), ('hu', 1.0), ('hy', 1.0), ('id', 1.0), ('it', 1.0), ('ja', 1.0), ('kk', 1.0), ('kmr', 1.0), ('ko', 1.0), ('la', 1.0), ('lv', 1.0), ('nl', 1.0), ('no_bokmaal', 1.0), ('no_nynorsk', 1.0), ('pl', 1.0), ('pt', 1.0), ('ro', 1.0), ('ru', 1.0), ('sk', 1.0), ('sl', 1.0), ('sme', 1.0), ('sr', 1.0), ('sv', 1.0), ('tr', 1.0), ('ug', 1.0), ('uk', 1.0), ('ur', 1.0), ('vi', 1.0), ('zh', 1.0)]\n",
    "```\n",
    "\n",
    "To **delete a local model**, run:\n",
    "``\n",
    "model_store_object.delete_model(lang_code = \"my_model\", version = \"1.0\")\n",
    "``\n",
    "or adjust parameters accordingly. Note that the associated embeddings file will be deleted too, unless used by at least one other existing model. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
